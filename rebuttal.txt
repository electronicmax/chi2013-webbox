We would like to thank the reviewers & AC for their extremely detailed suggestions and comments.  In this rebuttal, we will briefly address the suggestions and concerns in order of importance (as described by the AC): evaluation, prestudy 1 sample bias, similarity to "mash-up" makers, potential for automation, and, paper formatting & figures.

Evaluation - R2's comments suggest that the description of the study design was perhaps too parsimonious, and should be made clearer. The extra space gained from condensing Table 1 (and possibly eliminating Table 3 altogether, as suggested by R2) should give us space to adequately describe the tasks and methodology, as well as to provide some design justification to the choice of tasks and format, including choice of measures. We believe R2's comments also indicate a need to clarify the ways each of the measures were taken; for example, we will clarify that the number of properties considered were measured by counting distinct properties mentioned during the think-aloud process. We can also provide a discussion of the potential limitations of such approaches (e.g., of think-aloud for task analysis, and how we think this technique may under-estimate results but may be still suitable for comparative analysis used consistently across conditions). Addressing the relevance of evaluation (R2), we would like to improve the descriptive text that connects the hypotheses (h1-3) with the goals set out in the intro, and similarly, the measures taken with the hypotheses. We will also call out the potential bias resulting from the subject selection process of PS1 (as suggested by R1) which involved 5 computer scientists - a substantially different source population than that for our main study.

Distinction with "mashup-makers" - While we intended to cite Mashmaker, Vegemite & Marmite, we omitted them due to perceived differences in emphasis & approach. First, these systems focused on unstructured data acquisition, e.g., scraping and collecting data from web pages. With DataPalette, we started with pre-structured data from APIs and feeds allowing for a focus on structured reconciliation problems instead. Second, DataPalette gets users to work directly with instances, while Vegemite, Marmite & Yahoo! Pipes required users to construct data-flows (using visual programming paradigms) or scripts (through programming by demonstration). We believe that directly working with instances is simpler and more accessible, although perhaps more limited, than these "simplified programming" approaches, because users avoid having to think about generalizing their actions using abstract operators. A direct-manipulation approach also better supports small-scale exploratory sensemaking by providing direct, immediate feedback. We will to mention these differences, & re-introduce references as suggested.

Automation - We agree w/ R1 & R3 that automation could improve DataPalette and potentially help people arrive at conclusions faster. We considered a number of approaches to automating same-as & view selection during the design process, but ran into challenges. First, testers only sometimes wanted the consolidate data from multiple places, on a seemingly task & individual basis. Un-consolidated instances were preferred sometimes for reasons of simplicity "it's easier to think about it separately", or for reasons of reputation & trust (e.g., "Yelp reviews are better so I separate them"). Thus, even if reconciliation were automatic, selectively (manually) initiating it might be important. The second, greater challenge pertained to error handling: errors in consolidation were difficult to detect. Even with our manual-only approach, discovering and correcting incorrectly-combined (false positives) or incompletely-combined instances (e.g., resulting from dragging & dropping items in the wrong place(s)) caused people to be confused. Thus we must consider ways to make this combination process more visible and understandable. In summary, re: automation, we think that there is much potential for automation, but it also  presents substantial interaction challenges we need to understand better.  We will mention these challenges in "Future work" at the end of the paper.

Formatting and figures - In order to make figures more readable (AC), we will expand to full-width the screen shot in Fig.1, and make the fonts of Fig.2 and Fig.5 larger as well. As described, we will consolidate T1 and eliminate T3 to make room for the above mentioned improvements. In its place, we will (slightly) expand the textual discussion of PS2 (originally explanatory text for T1) to focus on observations that most informed the design of the system. We will perform additional proofreading and corrections, including the minor edits suggested by R2 and R3.

In summary, we hope to provide a clearer, more readable and better justified final copy, using space gained from Tables 1 & 3, and improving the final discussion of results. 
