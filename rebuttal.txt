We would like to thank the reviewers & AC for their extremely detailed suggestions and comments. We would like to address the suggestions and concerns in order of importance suggested by the AC as follows: evaluation and discussion, similarity to "mash-up" makers, lack of automation, formatting and figures, and future work.

Evaluation - We think R2's comments suggest that our description of the tasks and experimental set-up were perhaps too parsimonious, and could be expanded to be made clearer.  The extra space that will be gained from condensing Table 1 (and possibly eliminating Table 3 altogether, as suggested by R2) will allow us to describe the tasks and methodology adequately, as well as to go into our task design justification with greater detail.  We believe that R2's comments also indicate a need to expand the description of how measures were taken, particularly pertaining to task performance (e.g. that the number of dimensions each participant considered was derived from an analysis of  speak-aloud dictation). Addressing the relevance of evaluation (R2), plan to connect with better descriptive text, the hypotheses (h1-3) with the goals initially set out in the introduction, and similarly, the measures taken with the hypotheses.

Distinction with "mashup-makers" - While we originally cited the Mashmaker, Vegemite and Marmite work, in the process of condensing related/background work, decided to cut these due to difference in emphasis of capability and interface metaphors/data flow.  Seeking to enable reuse of data "off of any web page", a major capability of mashup makers is unstructured data acquisition, e.g., page scraping, tabular aggregation and shaping.  DataPalette, meanwhile, starts with the observation that already structured data can be acquired directly from most, if not all major web sites and platform providers today.  Second, 

Automation - We completely agree with R1 and R3 that careful introduction of automation could improve the user experience and reduce tedium, potentially helping people arrive at the view(s) and conclusions they want faster.  In fact, we tried a few approaches at automatic same-as reconciliation and visualisation view-inference (DWIM approaches) during our prototype design phase, and ran into a hurdles which essentially caused us to go back to the largely manual design for the final evaluation. 

The first issue pertained to our observation that participants only sometimes wanted consolidated views of data -- depending on task (and sometimes even on a per-instance basis).  Un-aggregated views were preferred for reasons ranging from "it's simply easier to think about 'facebook data' vs 'what's in my address book'", to reasons of provenance ("Yelp is more trustworthy so I want to refer to it separately"). Thus, even if reconciliation was made partially automatic, it would need to be user-initiated, so that support for separated (un-aggregated) views was maintained.  
The second issue pertained to helping users cope with errors -- that debugging incorrectly-combined (false positives) or left-separated (false negatives) causes immediate confusion for non-programmers.  We observed this during testing even without automatic aggregation, when participants inadvertantly combined records when accidentally dragging and dropping items - they simply had no idea what they did, and later could not figure out why dissimilar data items were combined.  

In summary we think that thus, intorduction of automation bears further design investigation, and will introduce these thoughts and possible workarounds in a "Future Work" section following "Study Limitations" 

